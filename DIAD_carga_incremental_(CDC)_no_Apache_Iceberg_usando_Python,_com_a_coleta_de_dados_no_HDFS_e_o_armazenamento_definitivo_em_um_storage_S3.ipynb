{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3/xK5gFa7VITOrhsESDr4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricardoricrob76/codigocoleta/blob/master/DIAD_carga_incremental_(CDC)_no_Apache_Iceberg_usando_Python%2C_com_a_coleta_de_dados_no_HDFS_e_o_armazenamento_definitivo_em_um_storage_S3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui3zS1N5_Zr3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para criar uma carga incremental (CDC) no Apache Iceberg usando Python, com a coleta de dados no HDFS e o armazenamento definitivo em um storage S3, você pode seguir os passos abaixo:\n",
        "\n",
        "Instale as bibliotecas necessárias: **negrito**\n",
        "Certifique-se de ter as bibliotecas necessárias instaladas. Você precisará do pacote pyarrow para manipular os dados no HDFS e do pacote s3fs para trabalhar com o S3. Você pode instalá-los usando o pip:\n",
        "\n"
      ],
      "metadata": {
        "id": "6kv8239pHqnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyarrow s3fs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs5hKrwZHvlb",
        "outputId": "ea63e002-06a4-41bc-b4c0-62ed5dada128"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (9.0.0)\n",
            "Collecting s3fs\n",
            "  Downloading s3fs-2023.5.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.22.4)\n",
            "Collecting aiobotocore~=2.5.0 (from s3fs)\n",
            "  Downloading aiobotocore-2.5.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec==2023.5.0 (from s3fs)\n",
            "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1 (from s3fs)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.29.77,>=1.29.76 (from aiobotocore~=2.5.0->s3fs)\n",
            "  Downloading botocore-1.29.76-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore~=2.5.0->s3fs) (1.14.1)\n",
            "Collecting aioitertools>=0.5.1 (from aiobotocore~=2.5.0->s3fs)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.29.77,>=1.29.76->aiobotocore~=2.5.0->s3fs)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.29.77,>=1.29.76->aiobotocore~=2.5.0->s3fs) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.29.77,>=1.29.76->aiobotocore~=2.5.0->s3fs) (1.26.15)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.29.77,>=1.29.76->aiobotocore~=2.5.0->s3fs) (1.16.0)\n",
            "Installing collected packages: multidict, jmespath, fsspec, frozenlist, async-timeout, aioitertools, yarl, botocore, aiosignal, aiohttp, aiobotocore, s3fs\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.4.0\n",
            "    Uninstalling fsspec-2023.4.0:\n",
            "      Successfully uninstalled fsspec-2023.4.0\n",
            "Successfully installed aiobotocore-2.5.0 aiohttp-3.8.4 aioitertools-0.11.0 aiosignal-1.3.1 async-timeout-4.0.2 botocore-1.29.76 frozenlist-1.3.3 fsspec-2023.5.0 jmespath-1.0.1 multidict-6.0.4 s3fs-2023.5.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bBc3hC8UHzxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install iceberg.api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVvz0I39KPUC",
        "outputId": "d8ff0148-6133-4a20-b106-4d38a38f2d40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement iceberg.api (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for iceberg.api\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importe os módulos necessários:\n",
        "Importe os módulos necessários para trabalhar com o Apache Iceberg, o HDFS e o S3:"
      ],
      "metadata": {
        "id": "zwvuFJOsH76a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyarrow import fs\n",
        "from pyarrow import dataset\n",
        "import s3fs\n",
        "import pyarrow.parquet as pq\n",
        "#from iceberg.api import FileFormat, Schema\n",
        "#from iceberg.hive import HiveCatalog\n"
      ],
      "metadata": {
        "id": "YYt94MOqH9tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure as credenciais do S3:\n",
        "Configure as credenciais do S3 para acessar o seu bucket:"
      ],
      "metadata": {
        "id": "gdw2Oe4JKvOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s3_access_key = 'YOUR_ACCESS_KEY'\n",
        "s3_secret_key = 'YOUR_SECRET_KEY'\n",
        "s3_bucket = 'YOUR_S3_BUCKET'\n"
      ],
      "metadata": {
        "id": "VTPKlBaMKy9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Oe6RhOOLKuPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crie a tabela Iceberg:\n",
        "Utilize a biblioteca HiveCatalog para criar a tabela Iceberg no HDFS. Certifique-se de fornecer o caminho HDFS desejado e o nome da tabela:"
      ],
      "metadata": {
        "id": "jaS8pqWpLSxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hdfs_path = 'hdfs://your_hdfs_path'\n",
        "table_name = 'your_table_name'\n",
        "\n",
        "catalog = HiveCatalog.builder().with_hadoop_conf().build()\n",
        "table = catalog.create_table(\n",
        "    Schema([\n",
        "        ('timestamp_col', 'timestamp'),\n",
        "        ('data_col', 'string')\n",
        "    ]),\n",
        "    hdfs_path + '/' + table_name,\n",
        "    format_=FileFormat.PARQUET\n",
        ")\n"
      ],
      "metadata": {
        "id": "hj2Mfo_FK8Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certifique-se de fornecer o esquema correto para a tabela e configurar o caminho HDFS desejado.\n",
        "\n",
        "Capture os dados incrementais no HDFS:\n",
        "Utilize a biblioteca pyarrow para capturar os dados incrementais e gravá-los no HDFS. Aqui está um exemplo básico:"
      ],
      "metadata": {
        "id": "HpuWCNsTLX6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "staging_path = 'hdfs://your_staging_path'\n",
        "\n",
        "# Capture os dados incrementais e salve-os em um arquivo Parquet\n",
        "# Você precisa implementar a lógica de captura dos dados incrementais de acordo com a sua fonte de dados\n",
        "incremental_data = [\n",
        "    {'timestamp_col': '2023-01-01 10:00:00', 'data_col': 'Incremental data 1'},\n",
        "    {'timestamp_col': '2023-01-01 11:00:00', 'data_col': 'Incremental data 2'}\n",
        "]\n",
        "df = pd.DataFrame(incremental_data)\n",
        "table_path = staging_path + '/' + table_name + '.parquet'\n",
        "pq.write_table(pa.Table.from_pandas(df), table_path)\n"
      ],
      "metadata": {
        "id": "mTdZKoikLb4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certifique-se de adaptar a lógica de captura dos dados incrementais de acordo com a sua fonte de dados.\n",
        "\n",
        "Realize a carga incremental na tabela Iceberg:\n",
        "Utilize a biblioteca pyarrow para carregar os dados incrementais na tabela Iceberg:"
      ],
      "metadata": {
        "id": "lOfwMj07LfmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table.load_in_place([table_path])\n"
      ],
      "metadata": {
        "id": "iUt_DiSCLloB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copie os dados para o S3:\n",
        "Utilize a biblioteca s3fs para copiar os dados do HDFS para o S3:"
      ],
      "metadata": {
        "id": "7sWBx3QHLoAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s3 = s3fs.S3FileSystem(key=s3_access_key, secret=s3_secret_key)\n",
        "s3_table_path = 's3://' + s3_bucket + '/' + table_name\n",
        "s3fs.du(s3_table_path)  # Verifique se o caminho S3 já existe\n",
        "s3fs.cp(table_path, s3_table_path, s3=s3)  # Copie os dados do HDFS para o S3\n"
      ],
      "metadata": {
        "id": "mtd3Lt8WLsgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certifique-se de fornecer suas próprias credenciais do S3 e configurar o caminho S3 desejado.\n",
        "\n",
        "Dessa forma, você pode criar uma carga incremental (CDC) no Apache Iceberg usando Python, coletando os dados no HDFS e armazenando-os de forma definitiva em um storage S3. Certifique-se de adaptar o código de acordo com suas necessidades específicas."
      ],
      "metadata": {
        "id": "jArx4evULxBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k6PNucJDLn7I"
      }
    }
  ]
}