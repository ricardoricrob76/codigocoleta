{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lbMWOUlqKXAX"
      },
      "outputs": [],
      "source": [
        "# Migração de Tabelas Hive para Tabelas Iceberg em ambiente hadoop com big Data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PASSO 01 - Instação de Dependências - Certifique-se de ter instaldo as bibliotecas necessárias, como PyArrow e Iceberg, no ambiente Python.  "
      ],
      "metadata": {
        "id": "s79IuxtwLxF6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyArrow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j63yJTRMCp5",
        "outputId": "e622ae84-36ed-45ef-d0e5-990e23721ac1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyArrow in /usr/local/lib/python3.10/dist-packages (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from PyArrow) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Iceberg"
      ],
      "metadata": {
        "id": "tzvWk_c6MFxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PASSO 02 - conectar ao Hive - Usando a Biblioteca PyHive para recuperar as tabelas existentes e seus esquemas."
      ],
      "metadata": {
        "id": "Sz0uBR5nMJO5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyhive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7M4nAYJMYSw",
        "outputId": "e7554df9-44a5-49c4-cdf3-86792d419007"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyhive\n",
            "  Downloading PyHive-0.6.5.tar.gz (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyhive) (0.18.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from pyhive) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->pyhive) (1.16.0)\n",
            "Building wheels for collected packages: pyhive\n",
            "  Building wheel for pyhive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyhive: filename=PyHive-0.6.5-py3-none-any.whl size=51554 sha256=0f25383548f14ad3979db15b1c5b1b39723b9c47567571c7637fe2ea4cdcea2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/51/26/016e93a30481dee1a91808520eefde1fff4da0804f289ac708\n",
            "Successfully built pyhive\n",
            "Installing collected packages: pyhive\n",
            "Successfully installed pyhive-0.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install thrift"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsJAeOc5Mlvh",
        "outputId": "152b9e27-cbe7-475b-a738-ae14c18cab4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting thrift\n",
            "  Downloading thrift-0.16.0.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.10/dist-packages (from thrift) (1.16.0)\n",
            "Building wheels for collected packages: thrift\n",
            "  Building wheel for thrift (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thrift: filename=thrift-0.16.0-cp310-cp310-linux_x86_64.whl size=399060 sha256=7e704e77232a73881390ad8d65106ab9d67e039f33e5b60b8468b0805a0b23e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/f8/d2/acfd995e8247eb0cad372fa6a640a5fcf279ab2ed7c5c4490e\n",
            "Successfully built thrift\n",
            "Installing collected packages: thrift\n",
            "Successfully installed thrift-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyhive import hive"
      ],
      "metadata": {
        "id": "eryDZ5u4MS1o"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# conectar ao Hive\n",
        "hive_conn = hive.Connection(host='localhost', port=10000, username='hadoop_user', passwd='senha@122')\n",
        "cursor = hive_conn.cursor()\n",
        "\n",
        "# Recuperar as tabelas existesntes no Hive\n",
        "cursor.execute(\"SHOW TABLES\")\n",
        "tables = cursor.fetchall()\n"
      ],
      "metadata": {
        "id": "107nv-KtMW_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PASSO 03 - Conectando com Tabelas ICeberg.\n",
        "from pyarrow import fs\n",
        "from pyarrow.dataset import dataset\n",
        "from pyarrow import Table\n",
        "from pyarrow import schema\n",
        "from pyarrow import iceberg\n",
        "\n",
        "# configurando o cliente no HDFS\n",
        "hdfs = fs.HadoopFileSystem(host='dominio.com.br', port=8020, user='hadoop_user', passwd='senha@123')\n",
        "\n",
        "for table_name in tables:\n",
        "  # Obter o esquema da tabela Hive\n",
        "  cursor.execute(f\"DESCRIBE {table_name}\")\n",
        "  hive_schema = cursor.fetchall()\n",
        "\n",
        "  # Criar o esquema Arrow a partir do esquema Hive\n",
        "  arrow_schema = schema.Schema.from_pandas(hive_schema) # Ajustes conforme necessidade.\n",
        "\n",
        "  # Criar a tabela Iceberg correspondente\n",
        "  iceberg_table_location = f'hdfs:/home/data_lakehouse/INSS/dadossaida/iceberg_table/{table_name}'\n",
        "  iceberg_table = iceberg.Table.create(iceberg_table_location, schema=arrow_schema)\n",
        "\n",
        "  # Obter os dados da Tabela Hive\n",
        "  cursor.execute(f\"SELECT * FROM {table_name}\")\n",
        "  hive_data = cursor.fetchall()\n",
        "\n",
        "  # converter os dados Hive para Arrow Table\n",
        "  arrow_data = Table.from_pandas(hive_data, schema=arrow_schema)\n",
        "\n",
        "  # Criar um novo arquivo Iceberg\n",
        "  new_file = iceberg_table.new_data_file()\n",
        "\n",
        "  # Configurar o escritor Iceberg\n",
        "  with new_file.new_row_write() as writer:\n",
        "    # Gravar os dados na tabela Iceberg\n",
        "    writer.write_table(arrow_data)\n",
        "\n",
        "  # Adicionar o novo arquivo à tabela Iceberg\n",
        "  iceberg_table.new_version().add_file(new_file).commit()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y8GPw_RzNVSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PASSO 04 - Validação e Testes\n",
        "# Após a migração, é importante validar os dados migrados para garantir que tudo ocorreu\n",
        "# conforme o esperado. Você pode consultar e verificar os dados na tabela Iceberg usando a\n",
        "# Biblioteca PyArrow."
      ],
      "metadata": {
        "id": "Ou-2y5hlR6s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar a tabela Iceberg migrada\n",
        "loaded_table = iceberg.Table.load(iceberg_table_location, hdfs=hdfs)\n",
        "\n",
        "# Ler os dados da table Iceberg\n",
        "iceberg_data = loaded_table.to_pandas()\n",
        "\n",
        "# Realizar tests e validações nos dados migrados\n",
        "# Comparar os dados originais no Hive, com os dados migrados para tabelas Iceberg."
      ],
      "metadata": {
        "id": "-4Yn0YMHSSsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}